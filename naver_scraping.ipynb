{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 매장정보 크롤링\n",
    "\n",
    "#### 자료 \n",
    "https://www.localdata.go.kr/devcenter/dataDown.do?menuNo=20001\n",
    "\n",
    "1. 관광식당, 일반음식점, 휴게음식점 데이터셋 다운로드(07_24_01_P_.csv.zip, 07_24_04_P_.csv.zip , 07_24_05_P_.csv.zip)\n",
    "2. 다운로드한 파일들을 data 폴더에 넣어주세요. (압축풀지말고)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제 \n",
    "1. 강남구 매장이지만, 네이버로 검색하는 경우 강남구가 아닌 다른 지역의 매장명이 검색됨\n",
    "2. 데이터 반영이 빠른편이지만, 네이버에 매장이 없는 경우가 있음 (없는 매장 : 맥도날드, 교촌, 굽네) \n",
    "3. 검색하면 전혀 다른 업체가 나오는 경우가 있음 -> 네이버 업태구분명을 크롤링해와서 참고하여 반영해야 \n",
    "4. 나라에서 제공하는 data를 사용하는 경우 인기도를 알기 어려움. 아닌가. -> 네이버 지도 api를 사용해서 점수를 부여할까? \n",
    "5. 자료 업데이트 시기 문제 - data를 다운로드 받아서 사용하는 경우, 매장 정보는 계속 바뀐는데 업데이트가 늦잖아... 매장 정보의 변동이 생기 사람이 계속 업로드를 해야 하잖아.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹 드라이버 설정\n",
    "from selenium import webdriver  \n",
    "from webdriver_manager.chrome import ChromeDriverManager \n",
    "\n",
    "# 대기 관련 라이브러리\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "\n",
    "# 예외 처리 관련 라이브러리\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException  \n",
    "\n",
    "# 웹 요소 찾기 관련 라이브러리\n",
    "from selenium.webdriver.common.by import By  \n",
    "from selenium.webdriver.support.ui import Select  \n",
    "from selenium.webdriver.common.keys import Keys  \n",
    "\n",
    "# 그 외 \n",
    "import time \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from bs4 import BeautifulSoup \n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import re  \n",
    "from tqdm import tqdm  # 반복문 진행 상황 시각화 모듈\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지역 설정\n",
    "city = '서울특별시'\n",
    "gu = '강남구'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip 파일 압축 풀기 \n",
    "\n",
    "folder_path = './data'\n",
    "zip_folder = './data/zip_data'\n",
    "\n",
    "# data 폴더 안에 있는 압축 파일들 압축 풀기\n",
    "for file in os.listdir(folder_path):\n",
    "    zip_file_path = os.path.join(folder_path, file)\n",
    "    if zipfile.is_zipfile(zip_file_path): \n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(zip_folder)\n",
    "\n",
    "# csv 파일이 큰 순서대로 정렬\n",
    "csv_files_sorted = sorted([os.path.join(zip_folder, f) for f in os.listdir(zip_folder) if f.endswith('.csv')],\n",
    "                          key=lambda x: os.path.getsize(x),reverse=True)\n",
    "csv_files_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫번째 데이터 불러오기 (데이터 크기가 커서 둘로 나눈뒤 합침)\n",
    "chunk_size = 1000000\n",
    "chunks = pd.read_csv(csv_files_sorted[0], encoding='cp949', encoding_errors='ignore', chunksize=chunk_size)\n",
    "df1_1 = next(chunks)  # 첫 번째 청크(1,000,000 행) 읽기\n",
    "df1_2 = pd.concat(chunks, ignore_index=True)  # 나머지 데이터 읽기\n",
    "\n",
    "# 현재 운영하고 있는 매장만 가져와서 합치기 \n",
    "df1_1.drop(df1_1[df1_1['영업상태명'] == '폐업'].index, inplace=True)\n",
    "df1_2.drop(df1_2[df1_2['영업상태명'] == '폐업'].index, inplace=True)\n",
    "df1 = pd.concat([df1_1, df1_2], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두번째 데이터 불러오기 \n",
    "df2 = pd.read_csv(csv_files_sorted[1], encoding='cp949', encoding_errors='ignore')\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세번째 데이터 불러오기 \n",
    "df3 = pd.read_csv(csv_files_sorted[2], encoding='cp949', encoding_errors='ignore')\n",
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 컬럼만 들고 오기 \n",
    "selected_columns = ['영업상태명', '소재지전체주소', '도로명전체주소', '사업장명', '최종수정시점','데이터갱신일자','업태구분명', '좌표정보(x)', '좌표정보(y)','위생업태명']\n",
    "\n",
    "df1 = df1[selected_columns]\n",
    "\n",
    "# 지역구 구분하기 \n",
    "df1['시도'] = df1['소재지전체주소'].apply(lambda x: x.split(' ', 1)[0] if pd.notna(x) else None)\n",
    "df1['지역구'] = df1['소재지전체주소'].apply(lambda x: x.split(' ')[1] if pd.notna(x) and len(x.split(' ')) > 1 else None)\n",
    "\n",
    "\n",
    "\n",
    "# 현재 운영하고 있는 매장만 가져오기\n",
    "df2.drop(df2[df2['영업상태명'] == '폐업'].index, inplace=True)\n",
    "\n",
    "# 필요한 컬럼만 들고 오기 \n",
    "df2 = df2[selected_columns]\n",
    "\n",
    "# 지역구 구분하기 \n",
    "df2['시도'] = df2['소재지전체주소'].apply(lambda x: x.split(' ', 1)[0] if pd.notna(x) else None)\n",
    "df2['지역구'] = df2['소재지전체주소'].apply(lambda x: x.split(' ')[1] if pd.notna(x) and len(x.split(' ')) > 1 else None)\n",
    "\n",
    "\n",
    "# 현재 운영하고 있는 매장만 가져오기\n",
    "df3.drop(df3[df3['영업상태명'] == '폐업'].index, inplace=True)\n",
    "\n",
    "# 필요한 컬럼만 들고 오기 \n",
    "df3 = df1[selected_columns]\n",
    "\n",
    "# 지역구 구분하기 \n",
    "df3['시도'] = df3['소재지전체주소'].apply(lambda x: x.split(' ', 1)[0] if pd.notna(x) else None)\n",
    "df3['지역구'] = df3['소재지전체주소'].apply(lambda x: x.split(' ')[1] if pd.notna(x) and len(x.split(' ')) > 1 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 합치고 중복제거\n",
    "origin_df = pd.concat([df1, df2, df3], ignore_index=True).drop_duplicates()\n",
    "origin_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 폴더 안에 있는 파일을 반복해서 df에 저장하고 싶었는데... 메모리 이슈 실패 \n",
    "\n",
    "# zip_folder = './data/zip_data'\n",
    "# chunk_size = 1000000\n",
    "# df_list = []\n",
    "# n = 1\n",
    "\n",
    "\n",
    "# for file in os.listdir(zip_folder):\n",
    "#     file_path = os.path.join(zip_folder, file)\n",
    "    \n",
    "#     # 파일 크기가 800MB 이상인지 확인\n",
    "#     if os.path.getsize(file_path) >= 800000000:\n",
    "#         chunks = pd.read_csv(file_path, encoding='cp949', encoding_errors='ignore', chunksize=chunk_size)\n",
    "#         df1_1 = next(chunks)  # 첫 번째 청크(1,000,000 행) 읽기\n",
    "#         df1_2 = pd.concat(chunks, ignore_index=True)  # 나머지 데이터 읽기\n",
    "\n",
    "#         # 현재 운영하고 있는 매장만 가져오기\n",
    "#         df1_1.drop(df1_1[df1_1['영업상태명'] == '폐업'].index, inplace=True)\n",
    "#         df1_2.drop(df1_2[df1_2['영업상태명'] == '폐업'].index, inplace=True)\n",
    "#         df_0 = pd.concat([df1_1, df1_2], ignore_index=True)\n",
    "#         df_list.append('df_0')\n",
    "\n",
    "#     else:\n",
    "#         for n in range(1, 4):\n",
    "#             df_name = 'df_{}'.format(n)\n",
    "#             globals()[df_name] = pd.read_csv(file_path, encoding='cp949', encoding_errors='ignore')\n",
    "#             df_list.append(globals()[df_name])\n",
    "                \n",
    "# df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '시도' 컬럼이 '서울특별시'면서 '지역구' 값이 '강남구'인 행만 필터링\n",
    "df = origin_df[(origin_df['시도'] == city ) & (origin_df['지역구'] == gu )]\n",
    "df.head()\n",
    "\n",
    "# csv 파일로 저장\n",
    "df.to_csv('filtered_data_{}{}.csv'.format(city,gu), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 업태구분명 확인\n",
    "unique_values = df['업태구분명'].unique()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카페, 술, 출장요리 류 삭제\n",
    "제외 = ['전통찻집','호프/통닭','뷔페식','출장조리','정종/대포집/소주방','이동조리', '감성주점','까페','라이브카페','키즈카페','커피숍','편의점', '일반조리판매','아이스크림', '떡카페', '철도역구내', '푸드트럭', '과자점', '다방', '관광호텔']\n",
    "for i in 제외 :\n",
    "    df = df.drop(df[df['업태구분명'] == i].index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test할 매장\n",
    "df_top10 = df.head(10)\n",
    "df_top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 셀레니움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# webdriver_manager를 사용하여 ChromeDriver 다운로드 및 설정\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "# 주소 이동\n",
    "url = 'https://map.naver.com/'\n",
    "driver.get(url)\n",
    "time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지역+메뉴 이름 조합으로 url에 미리 포함시키는 driver.get 함수 , count는 검색했을때 조회하는 범위\n",
    "def naver_finder(place,count) :\n",
    "\n",
    "  # 검색어를 네이버 url에 포함시켜 이동 + 검색 \n",
    "  driver.get('https://map.naver.com/p/search/{}'.format(place))\n",
    "\n",
    "  # 저장 \n",
    "  naver_df = pd.DataFrame()\n",
    "  time.sleep(3)  \n",
    "  try : \n",
    "    # 목록의 1~9번째까지 반복\n",
    "    for num in range(1,count+1) :\n",
    "      # frame을 3갤 나눔 searchIframe(왼쪽), entryIframe(오른쪽),default_content(기본)\n",
    "\n",
    "      # 왼쪽 프레임\n",
    "      searchIframe = driver.find_element(By.ID,'searchIframe')\n",
    "      driver.switch_to.frame(searchIframe)\n",
    "\n",
    "      try : \n",
    "        # {}안에 num 입력 (1번째, 2번째...)\n",
    "        # 각 카드 상단을 클릭하여 WebDriverWait을 활용 카드 상단 XPATH가 보일때까지 3초 대기 \n",
    "        driver.find_element(By.XPATH,'//*[@id=\"_pcmap_list_scroll_container\"]/ul/li[{}]/div[1]/a'.format(num)).click()\n",
    "\n",
    "      except :\n",
    "        # 가능하지 않다면 스크롤 내리고 최대 3초까지 기다렸다가 클릭\n",
    "        # tag 내용이 안 보이면 해당 tag에서 스크롤 내림\n",
    "        # click()이 안되는 경우 클릭을 대체하여 사용하기 \n",
    "        driver.find_element(By.TAG_NAME,'body').send_keys(Keys.PAGE_DOWN)\n",
    "        card_clik = driver.find_element(By.XPATH,'//*[@id=\"_pcmap_list_scroll_container\"]/ul/li[{}]/div[1]/a'.format(num))\n",
    "        driver.execute_script('arguments[0].click();',card_clik)\n",
    "\n",
    "      # 기본 프레임\n",
    "      time.sleep(1)\n",
    "      driver.switch_to.default_content()\n",
    "\n",
    "      # 오른쪽 프레임\n",
    "      entryIframe = driver.find_element(By.ID,'entryIframe')\n",
    "      driver.switch_to.frame(entryIframe)\n",
    "\n",
    "      # 가게 이름, 변수로 지정\n",
    "      restaurant_name = driver.find_element(By.CLASS_NAME,'Fc1rA').text\n",
    "\n",
    "      # 가게 별점이 있는 경우에만 try \n",
    "      try :\n",
    "        review_star = driver.find_element(By.CLASS_NAME,'PXMot').text\n",
    "        review_star = re.sub('방문자리뷰',\"\",review_star)\n",
    "      except :\n",
    "        review_star = 0\n",
    "        pass \n",
    "\n",
    "      # 기본 설정으로 돌아오기\n",
    "      time.sleep(1)\n",
    "      driver.switch_to.default_content()\n",
    "\n",
    "      print(restaurant_name,review_star)\n",
    "      \n",
    "      # 수정 필요 \n",
    "      df = pd.DataFrame({'가게명':[restaurant_name],'네이버별점' : [review_star]})\n",
    "      naver_df = pd.concat([naver_df,df])\n",
    "      naver_df.drop_duplicates() # 중복 제거 \n",
    "  except :\n",
    "    print('정상적으로 작동이 되지 않습니다.')\n",
    "\n",
    "    return naver_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정상적으로 작동이 되지 않습니다.\n",
      "정상적으로 작동이 되지 않습니다.\n",
      "정상적으로 작동이 되지 않습니다.\n",
      "정상적으로 작동이 되지 않습니다.\n",
      "정상적으로 작동이 되지 않습니다.\n",
      "하늘사다리 별점\n",
      "4.43\n",
      "은희네 온집닭떡볶이 상도본점 별점\n",
      "4.47\n",
      "정상적으로 작동이 되지 않습니다.\n",
      "정상적으로 작동이 되지 않습니다.\n",
      "정상적으로 작동이 되지 않습니다.\n",
      "정상적으로 작동이 되지 않습니다.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in df_top10['사업장명']:\n",
    "    df_sample = naver_finder(i,2)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
